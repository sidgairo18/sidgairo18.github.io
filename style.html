<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Style WACV 2020</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="images/teaser_fb.jpg"/>
<meta property="og:title" content="Unsupervised Image Style Embeddings for Retrieval and Recognition Tasks"/>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-120374008-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-120374008-1');
</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<body>

<div id="primarycontent">
<center><h1>Unsupervised Image Style Embeddings for Retrieval and Recognition Tasks</h1></center>
<center><h2>
	<a href="https://sidgairo18.github.io/">Siddhartha Gairola</a>&nbsp;&nbsp;&nbsp;
	<a href="https://cvit.iiit.ac.in/people/phd/phd-students/rajvi-shah">Rajvi Shah</a>&nbsp;&nbsp;&nbsp;
	<a href="https://faculty.iiit.ac.in/~pjn/">PJ Narayanan</a>&nbsp;&nbsp;&nbsp;
	</h2>
	<center><h2>
		<a href="https://iiit.ac.in/">IIIT Hyderabad</a>&nbsp;&nbsp;&nbsp;
	</h2></center>
<center><h2>in WACV 2020 (Poster and Oral)</h2></center>
<center><h2><strong><a href="https://">Paper</a> | <a href="https://">Code</a>  </strong> </h2></center> 
<center>
<img src="style_images/style_meanings.png" width="97%">
</center>
<p></p>

<p>

<br>

<h2 align="center">Abstract</h2>

<div style="font-size:14px"><p align="justify">We propose an unsupervised protocol for learning a neural embedding of visual style of images. Style similarity is an important measure for many applications such as style transfer, fashion search, art exploration, etc. However, computational modeling of style is a difficult task owing to its vague and subjective nature. Most methods for style based retrieval use supervised training with pre-defined categorization of images according to style. While this paradigm is suitable for applications where style categories are well-defined and curating large datasets according to such a categorization is feasible, in several other cases such a categorization is either ill-defined or does not exist. Our protocol for learning style based representations does not leverage categorical labels but a proxy measure for forming triplets of anchor, similar, and dissimilar images. Using these triplets, we learn a compact style embedding that is useful for style-based search and retrieval. The learned embeddings outperform other unsupervised representations for style-based image retrieval task on six datasets that capture different meanings of style. We also show that by fine-tuning the learned features with dataset-specific style labels, we obtain best results for image style recognition task on five of the six datasets.</p></div>

<a href="https://arxiv.org/abs/1903.07291"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="style_images/paper_thumbnail.png" width=170></a>



<h2>Paper</h2>
<p><a href="https://">arxiv</a>,  2029. </p>



<h2>Citation</h2>
<p>Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.<br>"Semantic Image Synthesis with Spatially-Adaptive Normalization", in CVPR, 2019.
<a href="SPADE.txt">Bibtex</a>


</p>

<h2>Code </h2> <p><a href='https://github.com/NVLabs/SPADE'> PyTorch </a></p>

<h1 align='center'> Brief Description of the Method and Motivation</h1>
<center></center>
<table width="100%" border="0" cellspacing="0" cellpadding="10" >
	<tr>
		<td width="50%" class="full">
			<img src="style_images/network_feats.png" style="width:100%;" align="middle">
            <p align="middle">Different feature layers of VGG-19 based CNN used for our experiments.</p>
		</td >
		<td width="50%" class="full">
			<img src="style_images/clustering.png" style="width:100%;" align="middle">
            <p align="middle">Triplet construction and selection process.</p>
		</td>
	</tr>
</table>
<br>
<p align="justify">One of the key motivations of this paper is to investigate the quality of understanding of style that can be achieved by an unsupervised approach which does not rely on categorical labels of style. To this effect, we evaluate state-of-the-art representations and their variants for style-based retrieval. We further propose a protocol for unsupervised learning of style representation by leveraging a proxy measure that provides a loose grouping of images. Our proxy measure is based on Gram matrix features popularized by style transfer methods. These features capture the <em>look and feel</em> of an image by measuring the correlation among feature maps produced by different convolutional layers of a CNN and hence are a good choice for discerning different visual styles. We train a Siamese CNN for learning a style embedding that is relevant for style based search and retrieval. However, instead of leveraging the style class labels specified for a dataset, we do this in an unsupervised fashion for many datasets. We first divide a dataset into <em>K</em> clusters using Gram matrix features and then use the cluster labels for learning the embedding by (i) directly minimizing a cross-entropy loss for cluster label classification, and (ii) minimizing a triplet loss for maximizing the distances between stylistically (look and feel wise) similar and dissimilar samples. The training with a triplet loss further reinforces the stylistic similarity which is depicted in Figure below. This is of large interest as the unsupervised protocol can be used on unlabelled (no supervision) data for learning stylistically useful representations and help understand a highly subjective concept like style (look and feel) better.
</p>


<br>
<h1 align='center'> Qualitative Results on Different Datasets </h1>
<p align="justify">Figures below show results of nearest neighbor retrieval for example queries from each dataset with triplet loss based representation <em>B-Tri</em> (refer paper for more details). Since style labels are often contextual and convey a limited meaning of style, a low precision score does not necessarily imply poor quality of visual similarity. The retrieved results that are highlighted by a black bounding box don't have the same style label as the query, despite obvious visual similarity.</p>

<br>
<h1 align='center'> BAM (Behance Artistic Media) Dataset Images </h1>
<center><img src="style_images/bam.png" width="1000"></center>
<p align="justify">Nearest Neighbour retrieval results for select queries from BAM subset test split. Notice that for rows 1 and 2, the queries and neighbours are very similar looking but the labels do not match. This indicates the lower mAP scores for retrieval using unsupervised methods. 'Oil Paint' and 'Water Colour' are hard to differentiate, similarly 'Gloomy' and 'Peaceful'.
</p>
<br>

<h1 align='center'> Flickr Dataset Images </h1>
<center><img src="style_images/flickr.png" width="1000"></center>
<p align="center"> Retrieval Results for Query and Top Neighbours Flickr Test Set.</p>
<br>

<h1 align='center'> Wikipaintings Dataset Images </h1>
<center><img src="style_images/wiki.png" width="1000"></center>
<p align="justify">Retrieval Results for Query and Top Neighbours Wikipaintings Subset dataset. It is interesting to see the retrieved results and their relevance with respect to the query image. Notice row 7 where, 'Abstract Expressionism' labelled query retrieves 'Ukiyo-e', 'Cubism' and 'Pop Art' paintings.</p>
<br>

<h1 align='center'> Ava Style Dataset Images </h1>
<center><img src="style_images/ava.png" width="1000"></center>
<p align="center"> Retrieval Results for Query and Top Neighbours Ava Style Test Set. </p>
<br>

<h1 align='center'> Deviant Art Dataset Images </h1>
<center><img src="style_images/deviant.png" width="1000"></center>
<p align="center"> Retrieval Results for Query and Top Neighbours Deviant Art Test Set. </p>
<br>

<h1 align='center'> Wall Art Dataset Images </h1>
<center><img src="style_images/wallart.png" width="1000"></center>
<p align="center"> Retrieval Results for Query and Top Neighbours Wall Art Test Set. </p>
<br>

<h1 align='center'> Code and Trained Models</h1>
	<p align="justify"> Please visit our <a href="https://github.com/">Github Repo</a>.  </p>

<br>

<h1>Acknowledgement</h1>
<p align="justify">We thank Alyosha Efros and Jan Kautz for insightful advice. Taesung Park contributed to the work during his internship at NVIDIA. His Ph.D. is supported by Samsung Scholarship. </p>

<br>
<h1>Related Work</h1>

<ul id='relatedwork'>
<div align="left">
<li font-size: 15px> V. Dumoulin, J. Shlens, and M. Kudlur. <a href="https://arxiv.org/abs/1610.07629"><strong>"A learned representation for artistic style"</strong></a>, in ICLR 2016.
</li>
<li font-size: 15px> H. De Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin, and A. C. Courville. <a href="https://arxiv.org/abs/1707.00683"><strong>"Modulating early visual processing by language"</strong></a>, in NeurIPS 2017.
</li>
<li font-size: 15px> T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. <a href="https://tcwang0509.github.io/pix2pixHD/"><strong>"High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs"</strong></a>, in CVPR 2018. (pix2pixHD)
</li>
<li font-size: 15px> P. Isola, J. Zhu, T. Zhou, and A. A. Efros. <a href="https://phillipi.github.io/pix2pix/"><strong>"Image-to-Image Translation with Conditional Adversarial Networks"</strong></a>, in CVPR 2017. (pix2pix)
</li>
<li font-size: 15px> Q. Chen and V. Koltun. <a href="https://cqf.io/ImageSynthesis/"><strong>"Photographic image synthesis with cascaded refinement networks.</strong></a>, ICCV 2017. (CRN)
</li>
</div>
</ul>

</body></html
>
